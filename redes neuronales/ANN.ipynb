{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a3gFOjqCCVa9"
   },
   "source": [
    "# Entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 4758,
     "status": "ok",
     "timestamp": 1608462980099,
     "user": {
      "displayName": "jonathan e",
      "photoUrl": "",
      "userId": "09935608602653684184"
     },
     "user_tz": 180
    },
    "id": "gR8q31fZjxk4"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils import data\n",
    "\n",
    "from google.colab import drive\n",
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si se trabaja local, ignorar la siguiente celda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27852,
     "status": "ok",
     "timestamp": 1608463003212,
     "user": {
      "displayName": "jonathan e",
      "photoUrl": "",
      "userId": "09935608602653684184"
     },
     "user_tz": 180
    },
    "id": "zCvdeAIwqljn",
    "outputId": "4a2be3ba-1cbf-4ada-8c0e-c7f5bbe2503b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "drive.mount('/content/drive')#, force_remount=True \n",
    "\n",
    "#drive.flush_and_unmount()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lectura de datos: en este caso se trabajó en colab, por lo que se debe correr la celda indicada más arriba y luego se modifica el file_path según el nombre de la carpeta de drive en la que estén almacenados los archivos checkpoint, loss y encoding2. \n",
    "En caso de trabajar local, se debe definir estas variables como *'Carpeta/NombreArchivo.extensión'*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_folder = 'drive/MyDrive/checkpoint3'\n",
    "loss_file = 'drive/MyDrive/loss3.txt'\n",
    "serv_enc = 'drive/MyDrive/encoding2.pickle'\n",
    "\n",
    "dataset_path = 'drive/MyDrive/401'\n",
    "train_path = dataset_path + '/train2.csv'\n",
    "val_path = dataset_path + '/val2.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación se definen los parámetros e hiperparámetros. Se pueden modificar según se estime conveniente.\n",
    "\n",
    "- learning_rate: paso con el cual la red aprende.\n",
    "- epochs: épocas de entrenamiento.\n",
    "- batch_size: número de 'divisiones' aleatorias que se realiza a los datos de entrenamiento para agilizar el proceso.\n",
    "- suffle: si se quiere modificar el orden de los datos.\n",
    "- log_interval: parámetro que se utiliza en el proceso de entrenamiento.\n",
    "- checkpoint_every: cada cuánto guardar un checkpoint (para luego no tener que correr todo desde cero)\n",
    "- alpha: hiperparámetro de la red neuronal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 822,
     "status": "ok",
     "timestamp": 1608463034576,
     "user": {
      "displayName": "jonathan e",
      "photoUrl": "",
      "userId": "09935608602653684184"
     },
     "user_tz": 180
    },
    "id": "RXJ-_iyPmpRy"
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "epochs = 2000\n",
    "batch_size = 256\n",
    "shuffle = True\n",
    "log_interval = 100\n",
    "checkpoint_every = 1\n",
    "alpha = 1/1000\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "torch.manual_seed(0)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 721,
     "status": "ok",
     "timestamp": 1608463036316,
     "user": {
      "displayName": "jonathan e",
      "photoUrl": "",
      "userId": "09935608602653684184"
     },
     "user_tz": 180
    },
    "id": "SJFfTc4-WSnk"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(checkpoint_folder):\n",
    "    os.makedirs(checkpoint_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 702,
     "status": "ok",
     "timestamp": 1608463038088,
     "user": {
      "displayName": "jonathan e",
      "photoUrl": "",
      "userId": "09935608602653684184"
     },
     "user_tz": 180
    },
    "id": "-ucRshLaj2ap"
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_size=16):\n",
    "      super(Net, self).__init__()\n",
    "\n",
    "      self.dropout1 = nn.Dropout(0.25)\n",
    "      self.dropout2 = nn.Dropout(0.25)\n",
    "      self.dropout3 = nn.Dropout(0.25)\n",
    "      self.dropout4 = nn.Dropout(0.25)\n",
    "      self.dropout5 = nn.Dropout(0.25)\n",
    "\n",
    "      self.fc1 = nn.Linear(input_size, 16)\n",
    "      self.fc2 = nn.Linear(16, 32)\n",
    "      self.fc3 = nn.Linear(32, 64)\n",
    "      self.fc4 = nn.Linear(64, 128)\n",
    "      self.fc5 = nn.Linear(128, 10)\n",
    "      self.fc6 = nn.Linear(10, 1)\n",
    "\n",
    "    # x represents our data\n",
    "    def forward(self, x):\n",
    "\n",
    "      x = F.relu(self.fc1(x))\n",
    "      x = self.dropout1(x)\n",
    "\n",
    "      x = F.relu(self.fc2(x))\n",
    "      x = self.dropout2(x)\n",
    "\n",
    "      x = F.relu(self.fc3(x))\n",
    "      x = self.dropout3(x)\n",
    "\n",
    "      x = F.relu(self.fc4(x))\n",
    "      x = self.dropout4(x)\n",
    "\n",
    "      x = F.relu(self.fc5(x))\n",
    "      x = self.dropout5(x)\n",
    "\n",
    "      output = self.fc6(x)\n",
    "      return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 669,
     "status": "ok",
     "timestamp": 1608463039570,
     "user": {
      "displayName": "jonathan e",
      "photoUrl": "",
      "userId": "09935608602653684184"
     },
     "user_tz": 180
    },
    "id": "jFrZp_Am05LO"
   },
   "outputs": [],
   "source": [
    "class RMSELoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.eps = eps\n",
    "        \n",
    "    def forward(self,yhat,y):\n",
    "        loss = torch.sqrt(self.mse(yhat,y) + self.eps)\n",
    "        return loss\n",
    "\n",
    "class CustomRMSELoss(nn.Module):\n",
    "    def __init__(self, alpha, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.eps = eps\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def forward(self,yhat,y):\n",
    "        loss = torch.sqrt(torch.mean(torch.exp(-self.alpha*y) * (yhat - y)**2) + self.eps)\n",
    "        #loss = torch.exp(-self.alpha*y)*torch.sqrt(self.mse(yhat,y) + self.eps)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 657,
     "status": "ok",
     "timestamp": 1608463042808,
     "user": {
      "displayName": "jonathan e",
      "photoUrl": "",
      "userId": "09935608602653684184"
     },
     "user_tz": 180
    },
    "id": "A4IIPYqFnhCy"
   },
   "outputs": [],
   "source": [
    "class MyDataset(data.Dataset):\n",
    "    def __init__(self, csv_path, serv_enc):\n",
    "        self.dataset = pd.read_csv(csv_path, delimiter=',')\n",
    "        self.length = self.dataset.shape[0]\n",
    "        with open(serv_enc, 'rb') as handle:\n",
    "          self.servs_encoding = pickle.load(handle)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "      row = self.dataset.iloc[index]\n",
    "      #row['Servicio'] = self.servs_encoding[row['Servicio']]\n",
    "      y = row[\"Diferencia\"].astype(np.float32)\n",
    "      x = row.drop(columns='Servicio').values[:15].astype(np.float32)\n",
    "      return torch.from_numpy(np.asarray(x)), torch.from_numpy(np.asarray(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 705,
     "status": "ok",
     "timestamp": 1608463044483,
     "user": {
      "displayName": "jonathan e",
      "photoUrl": "",
      "userId": "09935608602653684184"
     },
     "user_tz": 180
    },
    "id": "aL0MNBmeVO-e"
   },
   "outputs": [],
   "source": [
    "def save_model(net, optimizer, EPOCH, PATH, LOSS):\n",
    "  torch.save({\n",
    "              'epoch': EPOCH,\n",
    "              'model_state_dict': net.state_dict(),\n",
    "              'optimizer_state_dict': optimizer.state_dict(),\n",
    "              'loss': LOSS,\n",
    "              }, PATH)\n",
    "\n",
    "def load_model(PATH, optimizer, eval=False):\n",
    "  model = Net(input_size=15)\n",
    "\n",
    "  checkpoint = torch.load(PATH)\n",
    "  model.load_state_dict(checkpoint['model_state_dict'])\n",
    "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "  epoch = checkpoint['epoch']\n",
    "  loss = checkpoint['loss']\n",
    "\n",
    "  if eval:\n",
    "    model.eval()\n",
    "  else:\n",
    "    model.train()\n",
    "  return model.to(device), optimizer, epoch, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 8755,
     "status": "ok",
     "timestamp": 1608463054635,
     "user": {
      "displayName": "jonathan e",
      "photoUrl": "",
      "userId": "09935608602653684184"
     },
     "user_tz": 180
    },
    "id": "u0Ls_YC1Qkor"
   },
   "outputs": [],
   "source": [
    "train_dataset = MyDataset(train_path, serv_enc)\n",
    "train_loader = data.DataLoader(train_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=4, pin_memory=True)\n",
    "\n",
    "val_dataset = MyDataset(val_path, serv_enc)\n",
    "val_loader = data.DataLoader(val_dataset, batch_size=batch_size, shuffle=shuffle, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 12856,
     "status": "ok",
     "timestamp": 1608463064623,
     "user": {
      "displayName": "jonathan e",
      "photoUrl": "",
      "userId": "09935608602653684184"
     },
     "user_tz": 180
    },
    "id": "U5kcnPsOlwhP"
   },
   "outputs": [],
   "source": [
    "net = Net(input_size=15).to(device)\n",
    "\n",
    "# create a stochastic gradient descent optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=learning_rate)\n",
    "# create a loss function\n",
    "#criterion = nn.MSELoss()\n",
    "#criterion = RMSELoss()\n",
    "criterion = CustomRMSELoss(alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 684,
     "status": "ok",
     "timestamp": 1608463067070,
     "user": {
      "displayName": "jonathan e",
      "photoUrl": "",
      "userId": "09935608602653684184"
     },
     "user_tz": 180
    },
    "id": "VDGCV45dgAbx"
   },
   "outputs": [],
   "source": [
    "def train_model(train_loader, val_loader, net, optimizer, criterion, epochs, checkpoint_folder, epoch=None, log_interval=100, checkpoint_every=1):\n",
    "  begin = 0\n",
    "  if not epoch is None:\n",
    "    begin = epoch+1\n",
    "  epoch_loss = []\n",
    "  for epoch in range(begin, epochs):\n",
    "      running_loss = 0\n",
    "      for batch_idx, (x, y) in enumerate(train_loader):\n",
    "          x = x.to(device)\n",
    "          y = y.to(device)\n",
    "          optimizer.zero_grad()\n",
    "          net_out = net(x)\n",
    "          loss = criterion(torch.reshape(net_out, (-1,)), y)\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "\n",
    "          running_loss += loss.item( )\n",
    "          if batch_idx % log_interval == 0:\n",
    "              print('\\rTrain Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                      epoch, batch_idx * len(x), len(train_dataset),\n",
    "                            100. * batch_idx / len(train_loader), loss.item()), end='')\n",
    "      if epoch % checkpoint_every == 0:\n",
    "        save_model(net, optimizer, epoch, '{}/{}-{}.pt'.format(checkpoint_folder, epoch, time.time()), loss.item())\n",
    "      \n",
    "      error = eval_model(val_loader, criterion, net)\n",
    "      with open(loss_file, 'a+') as loss_:\n",
    "        loss_.write('{}, {}, {}\\n'.format(epoch, running_loss, error))\n",
    "      epoch_loss.append(running_loss)\n",
    "\n",
    "def eval_model(dataloader, criterion, model):\n",
    "  model.eval()\n",
    "  error = []\n",
    "  with torch.no_grad():\n",
    "    for batch_idx, (x, y) in enumerate(dataloader):\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "        net_out = model(x)\n",
    "        loss = criterion(torch.reshape(net_out, (-1,)), y)\n",
    "        error.append(loss.data.cpu().numpy())\n",
    "  model.train()\n",
    "  return np.mean(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8nI-8xe4gswH",
    "outputId": "610cf939-63ff-4faf-dc6b-2ed0aeb9781b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 32 [2739200/3072911 (89%)]\tLoss: 132.597366"
     ]
    }
   ],
   "source": [
    "#file_ = '12-1608224067.6807644.pt'\n",
    "#net, optimizer, epoch, loss = load_model('{}/{}'.format(checkpoint_folder, file_), optimizer, eval=False)\n",
    "epoch = None\n",
    "train_model(train_loader, val_loader, net, optimizer, criterion, epochs, checkpoint_folder, epoch=epoch, log_interval=log_interval, checkpoint_every=checkpoint_every)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S-EjyyWyaVW0"
   },
   "outputs": [],
   "source": [
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K1vFzr9AsX4s"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyNswt9++QtpjuCs+UlFpFqt",
   "collapsed_sections": [],
   "name": "cianciaDatos.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
